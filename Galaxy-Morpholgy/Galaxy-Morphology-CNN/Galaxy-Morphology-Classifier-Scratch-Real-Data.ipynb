{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, img_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    img_files = glob.glob(os.path.join(folder, '*.png'))\n",
    "    print(f\"Found {len(img_files)} images.\")\n",
    "    \n",
    "    for img_path in img_files:\n",
    "        filename = os.path.basename(img_path)\n",
    "        class_label = int(filename.split('_')[2].split('.')[0])  # Extract class from filename\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, img_size)\n",
    "            images.append(img)\n",
    "            labels.append(class_label)\n",
    "        else:\n",
    "            print(f\"Failed to load image: {img_path}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17736 images.\n",
      "Training samples: 14188\n",
      "Testing samples: 3548\n"
     ]
    }
   ],
   "source": [
    "base_folder = os.getcwd()\n",
    "data_folder = os.path.join(base_folder, 'Decals_data_images')\n",
    "img_size = (128, 128)  # Updated size\n",
    "\n",
    "X, y = load_images_from_folder(data_folder, img_size)\n",
    "\n",
    "X = X / 255.0\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "print(f'Training samples: {X_train.shape[0]}')\n",
    "print(f'Testing samples: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(42)\n",
    "        self.W1 = np.random.randn(3, 3, 3, 16).astype(np.float64) * 0.1\n",
    "        self.b1 = np.zeros((16,), dtype=np.float64)\n",
    "        self.W2 = np.random.randn(3, 3, 16, 32).astype(np.float64) * 0.1\n",
    "        self.b2 = np.zeros((32,), dtype=np.float64)\n",
    "        # Modified for 128x128 input: After two max-pooling layers, feature map will be 32x32\n",
    "        self.W3 = np.random.randn(32 * 32 * 32, 128).astype(np.float64) * 0.1\n",
    "        self.b3 = np.zeros((128,), dtype=np.float64)\n",
    "        self.W4 = np.random.randn(128, self.num_classes).astype(np.float64) * 0.1\n",
    "        self.b4 = np.zeros((self.num_classes,), dtype=np.float64)\n",
    "\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return expZ / expZ.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def max_pool(self, A, pool_size=2, stride=2):\n",
    "        n, h, w, c = A.shape\n",
    "        h_out = (h - pool_size) // stride + 1\n",
    "        w_out = (w - pool_size) // stride + 1\n",
    "        \n",
    "        output = np.zeros((n, h_out, w_out, c))\n",
    "        self.max_pool_masks = {}  \n",
    "        \n",
    "        for i in range(h_out):\n",
    "            for j in range(w_out):\n",
    "                h_start = i * stride\n",
    "                h_end = h_start + pool_size\n",
    "                w_start = j * stride\n",
    "                w_end = w_start + pool_size\n",
    "                \n",
    "                window = A[:, h_start:h_end, w_start:w_end, :]\n",
    "                window_reshaped = window.reshape(n, -1, c)\n",
    "                max_indices = window_reshaped.argmax(axis=1)\n",
    "\n",
    "                mask = np.zeros_like(window)\n",
    "                for batch in range(n):\n",
    "                    for channel in range(c):\n",
    "                        idx = max_indices[batch, channel]\n",
    "                        h_idx, w_idx = np.unravel_index(idx, (pool_size, pool_size))\n",
    "                        mask[batch, h_idx, w_idx, channel] = 1\n",
    "\n",
    "                self.max_pool_masks[(i, j)] = mask\n",
    "                output[:, i, j, :] = window_reshaped.max(axis=1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def max_pool_backward(self, dout, pool_size=2, stride=2):\n",
    "        n, h, w, c = dout.shape\n",
    "        h_out = h * stride\n",
    "        w_out = w * stride\n",
    "        dx = np.zeros((n, h_out, w_out, c))\n",
    "        \n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                h_start = i * stride\n",
    "                h_end = h_start + pool_size\n",
    "                w_start = j * stride\n",
    "                w_end = w_start + pool_size\n",
    "                \n",
    "                mask = self.max_pool_masks.get((i, j))\n",
    "                if mask is None:\n",
    "                    continue\n",
    "                    \n",
    "                # Ensure mask channels match output channels\n",
    "                if mask.shape[-1] != c:\n",
    "                    new_mask = np.zeros((n, pool_size, pool_size, c))\n",
    "                    min_channels = min(mask.shape[-1], c)\n",
    "                    new_mask[..., :min_channels] = mask[..., :min_channels]\n",
    "                    mask = new_mask\n",
    "                \n",
    "                # Broadcast the gradient\n",
    "                dx[:, h_start:h_end, w_start:w_end, :] += (\n",
    "                    mask * dout[:, i:i+1, j:j+1, :].reshape(n, 1, 1, -1)\n",
    "                )\n",
    "        \n",
    "        return dx\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.astype(np.float64)\n",
    "        \n",
    "        # Modified for 128x128 input\n",
    "        self.Z1 = np.zeros((X.shape[0], 128, 128, 16), dtype=np.float64)\n",
    "        for i in range(X.shape[0]):\n",
    "            for k in range(self.W1.shape[3]):\n",
    "                conv_result = np.zeros((128, 128), dtype=np.float64)\n",
    "                for c in range(3):\n",
    "                    kernel = self.W1[:, :, c, k]\n",
    "                    conv_result += cv2.filter2D(X[i, :, :, c], -1, kernel)\n",
    "                self.Z1[i, :, :, k] = conv_result\n",
    "        self.Z1 += self.b1\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "        self.P1 = self.max_pool(self.A1)  # Output: 64x64\n",
    "        \n",
    "        # Modified dimensions for second conv layer\n",
    "        self.Z2 = np.zeros((self.P1.shape[0], 64, 64, 32), dtype=np.float64)\n",
    "        for i in range(self.P1.shape[0]):\n",
    "            for k in range(self.W2.shape[3]):\n",
    "                conv_result = np.zeros((64, 64), dtype=np.float64)\n",
    "                for c in range(16):\n",
    "                    kernel = self.W2[:, :, c, k]\n",
    "                    conv_result += cv2.filter2D(self.P1[i, :, :, c], -1, kernel)\n",
    "                self.Z2[i, :, :, k] = conv_result\n",
    "        self.Z2 += self.b2\n",
    "        self.A2 = self.relu(self.Z2)\n",
    "        self.P2 = self.max_pool(self.A2)  # Output: 32x32\n",
    "        \n",
    "        self.F = self.P2.reshape(X.shape[0], -1)\n",
    "        self.Z3 = np.dot(self.F, self.W3) + self.b3\n",
    "        self.A3 = self.relu(self.Z3)\n",
    "        self.Z4 = np.dot(self.A3, self.W4) + self.b4\n",
    "        self.A4 = self.softmax(self.Z4)\n",
    "        \n",
    "        return self.A4\n",
    "\n",
    "    def conv2d_backward(self, input_data, kernel_size, output_gradient):\n",
    "        input_gradient = np.zeros_like(input_data)\n",
    "        kernel_gradient = np.zeros(kernel_size)\n",
    "        pad_size = (kernel_size[0] - 1) // 2\n",
    "    \n",
    "        padded_input = np.pad(input_data, \n",
    "                         ((0, 0), (pad_size, pad_size), \n",
    "                          (pad_size, pad_size), (0, 0)), \n",
    "                         mode='constant')\n",
    "\n",
    "        for i in range(input_data.shape[0]):\n",
    "            for c_out in range(output_gradient.shape[-1]):\n",
    "                for c_in in range(input_data.shape[-1]):\n",
    "                    kernel = np.flip(np.flip(self.W2[:, :, c_in, c_out], axis=0), axis=1)\n",
    "                    input_gradient[i, :, :, c_in] += cv2.filter2D(\n",
    "                        output_gradient[i, :, :, c_out].astype(np.float32),\n",
    "                        -1, \n",
    "                        kernel.astype(np.float32),\n",
    "                        borderType=cv2.BORDER_CONSTANT\n",
    "                    )\n",
    "    \n",
    "        for i in range(input_data.shape[0]):\n",
    "            for c_out in range(output_gradient.shape[-1]):\n",
    "                for c_in in range(input_data.shape[-1]):\n",
    "                    for h in range(kernel_size[0]):\n",
    "                        for w in range(kernel_size[1]):\n",
    "                            kernel_gradient[h, w, c_in, c_out] += np.sum(\n",
    "                                padded_input[i, h:h+output_gradient.shape[1], w:w+output_gradient.shape[2], c_in] *\n",
    "                                output_gradient[i, :, :, c_out]\n",
    "                            )\n",
    "    \n",
    "        return input_gradient, kernel_gradient\n",
    "    \n",
    "\n",
    "    def backward(self, X, Y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        dZ4 = self.A4 - Y\n",
    "        dW4 = np.dot(self.A3.T, dZ4) / m\n",
    "        db4 = np.sum(dZ4, axis=0) / m\n",
    "\n",
    "        dA3 = np.dot(dZ4, self.W4.T)\n",
    "        dZ3 = dA3 * (self.A3 > 0)\n",
    "        dW3 = np.dot(self.F.T, dZ3) / m\n",
    "        db3 = np.sum(dZ3, axis=0) / m\n",
    "        dF = np.dot(dZ3, self.W3.T).reshape(self.P2.shape)\n",
    "        dP2 = dF\n",
    "        dA2 = self.max_pool_backward(dP2)\n",
    "        dZ2 = dA2 * (self.A2 > 0)\n",
    "        \n",
    "        dP1, dW2 = self.conv2d_backward(self.P1, self.W2.shape, dZ2)\n",
    "        db2 = np.sum(dZ2, axis=(0, 1, 2)) / m\n",
    "        \n",
    "        dA1 = self.max_pool_backward(dP1)\n",
    "        dZ1 = dA1 * (self.A1 > 0)\n",
    "        \n",
    "        _, dW1 = self.conv2d_backward(X, self.W1.shape, dZ1)\n",
    "        db1 = np.sum(dZ1, axis=(0, 1, 2)) / m\n",
    "\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W3 -= learning_rate * dW3\n",
    "        self.b3 -= learning_rate * db3\n",
    "        self.W4 -= learning_rate * dW4\n",
    "        self.b4 -= learning_rate * db4\n",
    "\n",
    "    def train(self, X, Y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            Y_pred = self.forward(X)\n",
    "            loss = -np.mean(np.sum(Y * np.log(Y_pred + 1e-8), axis=1))\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n",
    "            self.backward(X, Y, learning_rate)\n",
    "\n",
    "    def evaluate(self, X, Y_true):\n",
    "        Y_pred = self.forward(X)\n",
    "        y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "        y_true_classes = np.argmax(Y_true, axis=1)\n",
    "        print(classification_report(y_true_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 111batch [1:04:15, 34.73s/batch, loss=2.23]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed in 3870.20 seconds\n",
      "Average loss: 2.3828\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 111batch [1:00:33, 32.74s/batch, loss=2.27]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed in 3654.35 seconds\n",
      "Average loss: 2.2439\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 111batch [59:58, 32.42s/batch, loss=2.25]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed in 3621.21 seconds\n",
      "Average loss: 2.2333\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 111batch [59:58, 32.42s/batch, loss=2.25]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed in 3620.97 seconds\n",
      "Average loss: 2.2211\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 111batch [59:46, 32.31s/batch, loss=2.24]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed in 3607.49 seconds\n",
      "Average loss: 2.2100\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 111batch [59:43, 32.28s/batch, loss=2.17]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed in 3604.42 seconds\n",
      "Average loss: 2.1999\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 111batch [59:46, 32.31s/batch, loss=2.16]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 completed in 3606.03 seconds\n",
      "Average loss: 2.1902\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 111batch [1:01:19, 33.14s/batch, loss=2.17]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed in 3701.60 seconds\n",
      "Average loss: 2.1821\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 111batch [1:00:01, 32.44s/batch, loss=2.13]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 completed in 3626.86 seconds\n",
      "Average loss: 2.1744\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 111batch [1:00:03, 32.46s/batch, loss=2.15]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed in 3629.96 seconds\n",
      "Average loss: 2.1663\n",
      "Total training time: 36562.38 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 78\u001b[0m\n\u001b[0;32m     68\u001b[0m train_with_parallel_batches_fast(\n\u001b[0;32m     69\u001b[0m     cnn, \n\u001b[0;32m     70\u001b[0m     X_train, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m  \u001b[38;5;66;03m# Increased batch size\u001b[39;00m\n\u001b[0;32m     75\u001b[0m )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Quick evaluation\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m pred_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     80\u001b[0m true_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 102\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mZ1 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mZ1)\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA1\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Output: 64x64\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Modified dimensions for second conv layer\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mZ2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m, in \u001b[0;36mSimpleCNN.max_pool\u001b[1;34m(self, A, pool_size, stride)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m channel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(c):\n\u001b[0;32m     48\u001b[0m         idx \u001b[38;5;241m=\u001b[39m max_indices[batch, channel]\n\u001b[1;32m---> 49\u001b[0m         h_idx, w_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munravel_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m         mask[batch, h_idx, w_idx, channel] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_pool_masks[(i, j)] \u001b[38;5;241m=\u001b[39m mask\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "def train_with_parallel_batches_fast(cnn, X_train, y_train, epochs, learning_rate, batch_size=256):\n",
    "    \"\"\"Faster training using parallel batch processing\"\"\"\n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert data to float32 for faster processing\n",
    "    X_train = X_train.astype(np.float64)\n",
    "    y_train = y_train.astype(np.float64)\n",
    "    \n",
    "    # Use multiple CPU cores for batch processing\n",
    "    num_cores = mp.cpu_count() // 2  # Use half of available cores\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        X_shuffled = X_train[indices]\n",
    "        y_shuffled = y_train[indices]\n",
    "        \n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Process batches with progress bar\n",
    "        with tqdm(total=num_batches, desc=f\"Epoch {epoch + 1}\", unit=\"batch\") as pbar:\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                batch_end = min(i + batch_size, num_samples)\n",
    "                X_batch = X_shuffled[i:batch_end]\n",
    "                y_batch = y_shuffled[i:batch_end]\n",
    "                \n",
    "                # Forward pass\n",
    "                Y_pred = cnn.forward(X_batch)\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(Y_pred + 1e-8), axis=1))\n",
    "                \n",
    "                # Backward pass\n",
    "                cnn.backward(X_batch, y_batch, learning_rate)\n",
    "                \n",
    "                epoch_losses.append(loss)\n",
    "                pbar.set_postfix(loss=loss)\n",
    "                pbar.update(1)\n",
    "        \n",
    "        epoch_end = time.time()\n",
    "        epoch_time = epoch_end - epoch_start\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f} seconds\")\n",
    "        print(f\"Average loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping if loss is good enough\n",
    "        if avg_loss < 0.1:  # Adjust this threshold based on your needs\n",
    "            print(\"Loss threshold reached, stopping early\")\n",
    "            break\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total training time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Usage:\n",
    "cnn = SimpleCNN(input_shape=(128, 128, 3), num_classes=y_train.shape[1])\n",
    "\n",
    "# Train with faster implementation\n",
    "train_with_parallel_batches_fast(\n",
    "    cnn, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=10,  # Reduced epochs\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128  # Increased batch size\n",
    ")\n",
    "\n",
    "# Quick evaluation\n",
    "y_pred = cnn.forward(X_test)\n",
    "pred_classes = np.argmax(y_pred, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(pred_classes == true_classes)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcnn\u001b[49m\u001b[38;5;241m.\u001b[39mforward(X_test)\n\u001b[0;32m      2\u001b[0m pred_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m true_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cnn' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = cnn.forward(X_test)\n",
    "pred_classes = np.argmax(y_pred, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(pred_classes == true_classes)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "model = SimpleCNN(input_shape=(128, 128, 3), num_classes=10)\n",
    "\n",
    "joblib.dump(model, 'cnn_model.pkl')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "loaded_model = joblib.load('cnn_model.pkl')\n",
    "print(\"Model loaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
